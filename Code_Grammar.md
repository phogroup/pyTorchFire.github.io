---
layout: page
mathjax: true
title: " "
---

# Code Grammar Configuration
### Code grammar

<div align="center"><img width="900" height="400" src="assets\Code_Gramar.jpg" /><div align="center">
<div align="left"> Left column: Firedrake grammar summary used to express loss functionals within TorchFire. We define our
domain as a union of polygonal patches, supporting unstructured hex/quad/triangle/tet meshes. The definition of entities
and function spaces includes attributes providing additional information that aids during code-generation. Expressions can
be defined using operators and entities. Finally, code is generated by specifying the target and host for a generation. Right
column: a working code example in TorchFire. In this case, u and κ are expected to be Torch tensors (given data and
predicted by the neural network, respectively) with size (nx + 1) × (ny + 1) = 256<div>

### Code example for solving hear equation demos

```python
# Step 0: Import necessary packges
from torchfire import fd_to_torch
import firedrake
import torch
from torch import nn
import pandas

# Step 1: Load train and test data
kappa_train, kappa_test, u_test = pandas.read_csv(...).to_numpy()

# Step 2: TorchFire physic module
## Firedrake Mesh and Space definition
nx, ny = 15, 15
mesh = UnitSquareMesh(nx, ny)
V = FunctionSpace(mesh, "P", 1)

## Boudary connditions
bc = DirichletBC(V, 0, (1, 2, 3))
## Weak form definition
def assemble_firedrake(u, kappa):
    x = SpatialCoordinate(mesh)
    v = TestFunction(u.function_space())
    f = Constant(20.0)
    return assemble(inner(exp(kappa) * grad(u), grad(v)) * dx - inner(f, v) * dx, bcs=bc)
    
## Torchfire wrapper for assemble_firedrake
templates = (firedrake.Function(V), firedrake.Function(V))
residualTorch = fd_to_torch(assemble_firedrake, templates, "residualTorch").apply

# Step 3: Neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, kappa_dim, u_dim, neurons):
        super(NeuralNetwork, self).__init__()
        self.Neuralmap1 = nn.Linear(kappa_dim, neurons)
        self.Relu = nn.ReLU()
        self.Neuralmap2 = nn.Linear(neurons, u_dim)
        self.MSE = nn.MSELoss()

    def forward(self, kappa):
        u = self.Neuralmap2(self.Relu(self.Neuralmap1(kappa)))
        return u
        
    def ResidualTorch(self, u, kappa):
        residuals = residualTorch(u, kappa)
        Physic_loss = self.MSE(residuals, torch.zeros_like(residuals))
        return Physic_loss
        
model = NeuralNetwork(kappa_dim = (nx+1)*(ny+1), u_dim = (nx+1)*(ny+1), neurons = 1000)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# Step 4 Define train and test function for learning
def train_loop(model, optimizer, kappa):
    train_loss = 0
    for batch in range(int(num_train / batch_size)):
        u_pred = model(kappa[(batch) * batch_size:(batch + 1) * batch_size, :])
        residuals = model.ResidualTorch(u_pred, kappa) / batch_size
        
        # Backpropagation
        optimizer.zero_grad(), residuals.backward(), optimizer.step(),
        train_loss += residuals
    return train_loss
        
def test_loop(model, kappa_test, u_test):
    with torch.no_grad():
        u_pred = model(kappa_test)
        test_acc = (torch.linalg.vector_norm(u_pred - u_test, dim=-1)**2
                    / torch.linalg.vector_norm(u_test, dim=-1)**2).mean()
    return test_acc

for _ in range(nEpochs):
    train_loss = train_loop(model, optimizer, kappa_train)
    test_acc = test_loop(model, kappa_test, u_test)
```

